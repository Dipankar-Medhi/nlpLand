{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_classification(clustering)_unsupervised_ML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing dataset and necessary libraries"
      ],
      "metadata": {
        "id": "V-8efIn6CO3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hbk7UAp47j71"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD \n",
        "from sklearn.pipeline import make_pipeline \n",
        "from sklearn.preprocessing import Normalizer    \n",
        "from sklearn import metrics   \n",
        "import numpy as np     "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(a_set, cats):    #B \n",
        "    dataset = fetch_20newsgroups(subset=a_set, categories=cats, \n",
        "                                remove=('headers', 'footers', 'quotes'), \n",
        "                                shuffle=True) \n",
        "    return dataset \n",
        "\n",
        "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \n",
        "\"rec.sport.baseball\",\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \n",
        "\"talk.politics.mideast\"] \n",
        "\n",
        "newsgroups_train = load_dataset('train', categories) \n",
        "newsgroups_test = load_dataset('test', categories)  "
      ],
      "metadata": {
        "id": "6VyTXLmu7rVz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "JZ1e991BDSRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "random.seed(42)    \n",
        "\n",
        "# combine training and test data into a single list\n",
        "all_news = list(zip(newsgroups_train.data, newsgroups_train.target)) \n",
        "all_news += list(zip(newsgroups_test.data, newsgroups_test.target))   \n",
        "# shuffle data randomly\n",
        "random.shuffle(all_news)    \n",
        "\n",
        "# store labels and contents separately\n",
        "all_news_data = [text for (text, label) in all_news] \n",
        "all_news_labels = [label for (text, label) in all_news]   \n",
        "\n",
        "print(\"Data:\") \n",
        "print(str(len(all_news_data)) + \" posts in \" \n",
        "     + str(np.unique(all_news_labels).shape[0]) + \" categories\\n\")    #E \n",
        "print(\"Labels: \") \n",
        "print(all_news_labels[:10]) \n",
        "num_clusters = np.unique(all_news_labels).shape[0] \n",
        "print(\"Actual number of clusters: \" + str(num_clusters))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h540AYvEWLk",
        "outputId": "dc3f2f86-e784-4ecc-9691-2eee6a57ecf4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:\n",
            "9850 posts in 10 categories\n",
            "\n",
            "Labels: \n",
            "[2, 6, 1, 9, 0, 5, 1, 2, 9, 0]\n",
            "Actual number of clusters: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "q0O3AJvgJ3WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ignore all words that occur in less than 2 documents or in \n",
        "# more than 50% documents. Also remote the stopwords and \n",
        "# apply inverse document frequency weights.\n",
        "vectorizer = TfidfVectorizer(min_df=2, max_df=0.5,    \n",
        "                            stop_words='english', \n",
        "                            use_idf=True)  \n",
        "\n",
        "def transform(data, vectorizer, dimensions): \n",
        "    trans_data = vectorizer.fit_transform(data) \n",
        "    print(\"Transformed data contains: \" + str(trans_data.shape[0]) + \n",
        "          \" with \" + str(trans_data.shape[1]) + \" features =>\")    \n",
        "    \n",
        "    # reduce the dimension of the data\n",
        "    svd = TruncatedSVD(dimensions)   \n",
        "    pipe = make_pipeline(svd, Normalizer(copy=False)) # normalizer helps adjust different ranges to same range\n",
        "    reduced_data = pipe.fit_transform(trans_data)    \n",
        "\n",
        "    return reduced_data, svd    \n",
        "\n",
        "reduced_data, svd = transform(all_news_data, vectorizer, 300) \n",
        "print(\"Reduced data contains: \" + str(reduced_data.shape[0]) + \n",
        "     \" with \" + str(reduced_data.shape[1]) + \" features\")   #H "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v07W8WyKF_r-",
        "outputId": "6ce76ba1-29a1-4c15-b238-232529f4934e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed data contains: 9850 with 33976 features =>\n",
            "Reduced data contains: 9850 with 300 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting"
      ],
      "metadata": {
        "id": "7X1W4IOqJ6zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans    \n",
        "\n",
        "def cluster(data, num_clusters): \n",
        "    km = KMeans(n_clusters=num_clusters, init='k-means++',    \n",
        "                max_iter=100, random_state=0)    \n",
        "    km.fit(data) \n",
        "\n",
        "    return km \n",
        "\n",
        "km = cluster(reduced_data, 10) "
      ],
      "metadata": {
        "id": "sPiUVLyZIEB8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the clusters"
      ],
      "metadata": {
        "id": "AuGz3stVJ_nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(km, labels, svd): \n",
        "    print(\"Clustering report:\\n\") \n",
        "\n",
        "    print(f\"* Homogeneity: {str(metrics.homogeneity_score(labels, km.labels_))}\") \n",
        "    print(f\"* Completeness: {str(metrics.completeness_score(labels, km.labels_))}\") \n",
        "    print(f\"* V-measure: {str(metrics.v_measure_score(labels, km.labels_))}\")  \n",
        "    print(\"\\nMost discriminative words per cluster:\") \n",
        "    # get centroids \n",
        "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)  \n",
        "    # sort the centroids  \n",
        "    order_centroids = original_space_centroids.argsort()[:, ::-1]  \n",
        "    # mapping centroids back to words  \n",
        "    terms = vectorizer.get_feature_names() \n",
        "\n",
        "    for i in range(num_clusters): \n",
        "        print(\"Cluster \" + str(i) + \": \") \n",
        "        cl_terms = \"\" \n",
        "        for ind in order_centroids[i, :50]: \n",
        "            cl_terms += terms[ind] + \" \" \n",
        "        print(cl_terms + \"\\n\")   \n",
        "        \n",
        "evaluate(km, all_news_labels, svd) \n",
        "print(\"\\nCategories:\") \n",
        "for i, category in enumerate(newsgroups_train.target_names): \n",
        "    print(\"*\", category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eroldZlpIbBY",
        "outputId": "3854e781-54eb-48e9-a8e9-17938fa1cc67"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering report:\n",
            "\n",
            "* Homogeneity: 0.4356706249829362\n",
            "* Completeness: 0.517119953001238\n",
            "* V-measure: 0.47291394000106934\n",
            "\n",
            "Most discriminative words per cluster:\n",
            "Cluster 0: \n",
            "don like just know people space think time good use does ve right years make way long things problem thing going work really sure say new want better used did probably high said doctor ll lot need didn orbit cause nasa idea point little earth help launch medical actually day \n",
            "\n",
            "Cluster 1: \n",
            "bike ride bikes riding just like motorcycle dod don ve road miles good honda got rear helmet turn right know really advice thing dog make left rider new engine going way time little work countersteering need buying passenger gear ll want used sure insurance did think stop shaft fast thanks \n",
            "\n",
            "Cluster 2: \n",
            "thanks edu com does mail just new think list know like good got looking right did say ve heard ll cars want information tell post sure really let time address make article used engine dod use way need send probably read thought david email info actually believe stuff university don \n",
            "\n",
            "Cluster 3: \n",
            "israel jews israeli armenian arab jewish people armenians turkish arabs muslims war muslim said killed government state genocide palestinian peace palestinians did just armenia turkey world turks rights israelis population land like soldiers human anti 000 right children fact soviet don serbs greek villages bosnian think know lebanon country nazi \n",
            "\n",
            "Cluster 4: \n",
            "sale 00 offer shipping condition asking new interested drive price email sell card 10 original 25 excellent mail edu used best 50 monitor 20 cd brand meg make software includes box manuals disk obo disks 15 following modem included model old like manual 40 power great ram hard 30 printer \n",
            "\n",
            "Cluster 5: \n",
            "key chip clipper encryption government keys nsa escrow algorithm use phone des security secure public people law crypto privacy data encrypted secret just don enforcement bit phones think chips number message know used using wiretap agencies scheme trust make like way rsa private serial time court fbi does right communications \n",
            "\n",
            "Cluster 6: \n",
            "geb cadre dsl n3jxp chastity skepticism pitt surrender shameful intellect gordon banks soon edu don lyme blood medical probably patients good weight usually won migraine think know sure disease need patient does brain people just pressure isn like meant time hope law dystrophy reflex effective getting wanted normal way sympathetic \n",
            "\n",
            "Cluster 7: \n",
            "car cars engine new good like ford just dealer miles know don speed think oil does drive ve time driving buy power price really convertible clutch driver 000 used problem tires year rear owner small insurance mileage manual make auto got right bought way better little looking toyota road want \n",
            "\n",
            "Cluster 8: \n",
            "game team games year hockey players season play think win baseball good don player league teams like time nhl espn did hit just fans better best really know series played years night going playing playoffs runs detroit boston great won toronto pens pitching goal braves ll leafs fan way didn \n",
            "\n",
            "Cluster 9: \n",
            "window server motif use using thanks windows file application display widget program code x11r5 running does sun set xterm problem like hi color help run x11 know version mit manager work openwindows ve error screen files way user source want client unix advance available lib just include appreciated need xlib \n",
            "\n",
            "\n",
            "Categories:\n",
            "* comp.windows.x\n",
            "* misc.forsale\n",
            "* rec.autos\n",
            "* rec.motorcycles\n",
            "* rec.sport.baseball\n",
            "* rec.sport.hockey\n",
            "* sci.crypt\n",
            "* sci.med\n",
            "* sci.space\n",
            "* talk.politics.mideast\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    }
  ]
}