{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_techniques.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Extract NOUN Phrases from text"
      ],
      "metadata": {
        "id": "zoAUsYo1lLqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K-MdHIMnlHIp"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import nltk\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional"
      ],
      "metadata": {
        "id": "LGQZxWucmevB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIofVD5bmG9N",
        "outputId": "7d00a402-f86b-46ab-9bb0-f58aaa194816"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract noun"
      ],
      "metadata": {
        "id": "LOONNlYvlha3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"Rahul is a great Machine Learning Engineer. He is specialized in Natural Language Processing.\")"
      ],
      "metadata": {
        "id": "KQX8EEyIlg2A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob.noun_phrases\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqtI7vwol5I-",
        "outputId": "a4719b27-547a-4205-e71e-84fb0f170035"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['rahul', 'machine learning engineer', 'language processing'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xLac14gOl5xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarities between words"
      ],
      "metadata": {
        "id": "C7zhU9g-mnpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Cosine similarity** -> cosine angle between two vectors.\n",
        "- **Jaccard similarity**\n",
        "- **Levenshtein distance**: Minimal number of \n",
        "insertions, deletions, and replacements required for \n",
        "transforming string “a” into string “b.”\n",
        "- **Hamming distance**: Number of positions with the \n",
        "same symbol in both strings. But it can be defined \n",
        "only for strings with equal length.\n"
      ],
      "metadata": {
        "id": "b_SaL9uVmsAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity"
      ],
      "metadata": {
        "id": "UAnS2rZmoyvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = ( \n",
        "\"I like Apple\",\n",
        "\"I am exploring Apple devices\",\n",
        "\"I am a beginner in Apple development\", \n",
        "\"I want to work for Apple\", \n",
        "\"I like Apple products\"\n",
        ")"
      ],
      "metadata": {
        "id": "OTSL5MTZmp-G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "e6bE82dFnzjL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfid_vectorizer = TfidfVectorizer()\n",
        "tfid_matrix = tfid_vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "3mjdtwGun-20"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfid_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X9H2_RBoNnl",
        "outputId": "28893c59-524e-481b-a9ae-5e87c361bd43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity of 1st sentence with rest of the sentences\n",
        "cosine_similarity(tfid_matrix[0:1], tfid_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbWK0lxcoSnV",
        "outputId": "7e435076-32b6-423a-bc53-124474d105ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.14284054, 0.12305308, 0.11786255, 0.68374784]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tzkhSJhQojrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Phonetic matchine"
      ],
      "metadata": {
        "id": "iDcV_1nUo1sC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It matches two works and create alphanumeric string as encode version of the word. It is great for matching relavant names."
      ],
      "metadata": {
        "id": "oFtWPaBopJMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXyS8f6vo4mD",
        "outputId": "838ef71d-3fa8-4d67-db4d-e3ed5aaade73"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzy\n",
            "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp37-cp37m-linux_x86_64.whl size=164017 sha256=6bd1982259720c570178b4cf52dffbd2e3fc99c7fc6ea8de07a5a93b3fcfbddf\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/52/8a/bb2d05fbf343752a8546682cb5b2d775cc0d1f27f6c43f95dd\n",
            "Successfully built fuzzy\n",
            "Installing collected packages: fuzzy\n",
            "Successfully installed fuzzy-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fuzzy"
      ],
      "metadata": {
        "id": "dxFZPXk5pZH0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soundex = fuzzy.Soundex(5)\n",
        "soundex(\"natural\")"
      ],
      "metadata": {
        "id": "9iWEvSEqpc7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IlRjjMrKpsRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part Of Speech tagging"
      ],
      "metadata": {
        "id": "ZGa59g5sqlVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Rule based\n",
        "- Stochastic based"
      ],
      "metadata": {
        "id": "kfWE5JVjquPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love programming. I love building softwares.\""
      ],
      "metadata": {
        "id": "9rcTSvHeqnxy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89JNiwvI0TGq",
        "outputId": "90cfd6dd-86c3-437a-baf3-2fb934372bc3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the text\n",
        "tokens = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "PmlyhaPl0iGI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAlgnc6s0sEg",
        "outputId": "7e822b0a-c0f7-4b66-9233-65c2ba70bad1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I love programming.', 'I love building softwares.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb28ZNzE1Gma",
        "outputId": "f113cfba-b47a-49f6-8e58-f272ef30f571"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tagging all the tokens\n",
        "for token in tokens:\n",
        "  words = nltk.word_tokenize(token)\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  # POS tagger\n",
        "  tags = nltk.pos_tag(words)\n",
        "\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJwC15HK0sjY",
        "outputId": "7f437ed3-d0f3-4ab7-fa20-1d93fb6c1c4a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('love', 'VBP'),\n",
              " ('building', 'VBG'),\n",
              " ('softwares', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All tags**\n",
        "\n",
        "- CC coordinating conjunction\n",
        "- CD cardinal digit\n",
        "- DT determiner\n",
        "- EX existential there (like: “there is” ... think of it like \n",
        "“there exists”)\n",
        "- FW foreign word\n",
        "- IN preposition/subordinating conjunction\n",
        "- JJ adjective ‘big’\n",
        "- JJR adjective, comparative ‘bigger’\n",
        "- JJS adjective, superlative ‘biggest’\n",
        "- LS list marker 1)\n",
        "- MD modal could, will\n",
        "- NN noun, singular ‘desk’\n",
        "- NNS noun plural ‘desks’107\n",
        "- NNP proper noun, singular ‘Harrison’\n",
        "- NNPS proper noun, plural ‘Americans’\n",
        "- PDT predeterminer ‘all the kids’\n",
        "- POS possessive ending parent’s\n",
        "- PRP personal pronoun I, he, she\n",
        "- PRP$ possessive pronoun my, his, hers\n",
        "\n",
        "- RB adverb very, silently\n",
        "- RBR adverb, comparative better\n",
        "- RBS adverb, superlative best\n",
        "- RP particle give up\n",
        "- TO to go ‘to’ the store\n",
        "- UH interjection\n",
        "- VB verb, base form take\n",
        "- VBD verb, past tense took\n",
        "- VBG verb, gerund/present participle taking\n",
        "- VBN verb, past participle taken\n",
        "- VBP verb, sing. present, non-3d take\n",
        "- VBZ verb, 3rd person sing. present takes\n",
        "- WDT wh-determiner which\n",
        "- WP wh-pronoun who, what\n",
        "- WP$ possessive wh-pronoun whose\n",
        "- WRB wh-adverb where, when\n"
      ],
      "metadata": {
        "id": "iWdiRsNp1TBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9DS8ap5H1CS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Entities from Text"
      ],
      "metadata": {
        "id": "IFkAvM--17YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify and extract entities from text, also called **Named Entity Recognition**."
      ],
      "metadata": {
        "id": "YXNlZXw92CNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Rahul is a very good footballer. He wants to play for his country.\""
      ],
      "metadata": {
        "id": "z9Zfot6t1-CO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "YdEndCH22e3i"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEG9K9sU21Qr",
        "outputId": "f858f26f-ca14-4fbe-8585-c69fdde6250e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NER\n",
        "ne_chunk(nltk.pos_tag(word_tokenize(text)), binary=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "rn908H272pCm",
        "outputId": "b54ceef4-c221-4a8d-9167-6085ec6981c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0msvgling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('GPE', [('Rahul', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('very', 'RB'), ('good', 'JJ'), ('footballer', 'NN'), ('.', '.'), ('He', 'PRP'), ('wants', 'VBZ'), ('to', 'TO'), ('play', 'VB'), ('for', 'IN'), ('his', 'PRP$'), ('country', 'NN'), ('.', '.')])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract topics from text"
      ],
      "metadata": {
        "id": "7L7Kc4DX4HdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is also known as topic modelling."
      ],
      "metadata": {
        "id": "6yMbAIT_4O10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"I am learning Natural Language Processing. And one day, I want to become a Machine Learning Eningeer\"\n",
        "text2 = \"Along with Machine Learning, I am also learning Pandas and Numpy and other data science stuffs\"\n",
        "\n",
        "docs = [text1, text2]\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3uS6iiT35n0",
        "outputId": "5d0ca477-e46d-43da-d97f-45da76992cd7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am learning Natural Language Processing. And one day, I want to become a Machine Learning Eningeer',\n",
              " 'Along with Machine Learning, I am also learning Pandas and Numpy and other data science stuffs']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "!pip install genism\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Ymf7u25EBV",
        "outputId": "73409470-448b-489a-d3d0-aefdcb6ba0a1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement genism (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for genism\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTpojEck6LD2",
        "outputId": "8c72e876-24c4-46d8-d7e3-9d8268749c27"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation) \n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "  stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "  punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "  normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
        "  return normalized\n",
        "\n",
        "docs_clean = [clean(doc).split() for doc in docs] \n",
        "docs_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBXka1Ly5QF0",
        "outputId": "06b7d91a-07c5-4b72-9de2-43744d6dd604"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['learning',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'one',\n",
              "  'day',\n",
              "  'want',\n",
              "  'become',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'eningeer'],\n",
              " ['along',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'also',\n",
              "  'learning',\n",
              "  'panda',\n",
              "  'numpy',\n",
              "  'data',\n",
              "  'science',\n",
              "  'stuff']]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import genism\n",
        "from genism import corpora\n",
        "\n",
        "# create term dictionary of corpus\n",
        "# where the unique terms are assigned an index\n",
        "dicts = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Convert list of docs into doc term\n",
        "# matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dicts.doc2bow(doc) for doc in doc_clean]\n",
        "doc_term_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "8Cjuqnh46WI4",
        "outputId": "95968b47-8eaf-4770-fc0c-0c618fc9363f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-a5ec4717e553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgenism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgenism\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create term dictionary of corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# where the unique terms are assigned an index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'genism'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the LDA model\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "# Running and Training LDA model on the document term matrix \n",
        "# for 3 topics.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dicts, passes=50)\n",
        "# Results\n",
        "print(ldamodel.print_topics())"
      ],
      "metadata": {
        "id": "L_0U3kyp68YS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}